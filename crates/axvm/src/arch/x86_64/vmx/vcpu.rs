use alloc::collections::VecDeque;
use core::fmt::{Debug, Formatter, Result};
use core::{arch::asm, mem::size_of};

use bit_field::BitField;
use x86::bits64::vmx;
use x86::dtables::{self, DescriptorTablePointer};
use x86::segmentation::SegmentSelector;
use x86_64::registers::control::{Cr0, Cr0Flags, Cr3, Cr4, Cr4Flags};

use super::as_axerr;
use super::structs::{MsrBitmap, VmxRegion};
use super::vmcs::{
    self, VmcsControl32, VmcsControl64, VmcsControlNW, VmcsGuest16, VmcsGuest32, VmcsGuest64,
    VmcsGuestNW, VmcsHost16, VmcsHost32, VmcsHost64, VmcsHostNW,
};
use super::VmxPerCpuState;
use crate::arch::{msr::Msr, ApicTimer, GeneralRegisters};
use crate::{AxvmHal, GuestPhysAddr, HostPhysAddr, NestedPageFaultInfo};
use axerrno::AxResult;

/// A virtual CPU within a guest.
#[repr(C)]
pub struct VmxVcpu<H: AxvmHal> {
    guest_regs: GeneralRegisters,
    host_stack_top: u64,
    vmcs: VmxRegion<H>,
    msr_bitmap: MsrBitmap<H>,
    apic_timer: ApicTimer<H>,
    pending_events: VecDeque<(u8, Option<u32>)>,
}

impl<H: AxvmHal> VmxVcpu<H> {
    pub(crate) fn new(
        percpu: &VmxPerCpuState<H>,
        entry: GuestPhysAddr,
        ept_root: HostPhysAddr,
    ) -> AxResult<Self> {
        let mut vcpu = Self {
            guest_regs: GeneralRegisters::default(),
            host_stack_top: 0,
            vmcs: VmxRegion::new(percpu.vmcs_revision_id, false)?,
            msr_bitmap: MsrBitmap::passthrough_all()?,
            apic_timer: ApicTimer::new(),
            pending_events: VecDeque::with_capacity(8),
        };
        vcpu.setup_msr_bitmap()?;
        vcpu.setup_vmcs(entry, ept_root)?;
        info!("[AxVM] created VmxVcpu(vmcs: {:#x})", vcpu.vmcs.phys_addr());
        Ok(vcpu)
    }

    /// Run the guest, never return.
    pub fn run(&mut self) -> ! {
        VmcsHostNW::RSP
            .write(&self.host_stack_top as *const _ as usize)
            .unwrap();
        unsafe { self.vmx_launch() }
    }

    /// Information for VM exits due to external interrupts.
    pub fn interrupt_exit_info(&self) -> AxResult<vmcs::VmxInterruptInfo> {
        vmcs::interrupt_exit_info()
    }

    /// Information for VM exits due to I/O instructions.
    pub fn io_exit_info(&self) -> AxResult<vmcs::VmxIoExitInfo> {
        vmcs::io_exit_info()
    }

    /// Basic information about VM exits.
    pub fn exit_info(&self) -> AxResult<vmcs::VmxExitInfo> {
        vmcs::exit_info()
    }

    /// Information for VM exits due to nested page table faults (EPT violation).
    pub fn nested_page_fault_info(&self) -> AxResult<NestedPageFaultInfo> {
        vmcs::ept_violation_info()
    }

    /// Guest general-purpose registers.
    pub fn regs(&self) -> &GeneralRegisters {
        &self.guest_regs
    }

    /// Mutable reference of guest general-purpose registers.
    pub fn regs_mut(&mut self) -> &mut GeneralRegisters {
        &mut self.guest_regs
    }

    /// Guest stack pointer. (`RSP`)
    pub fn stack_pointer(&self) -> usize {
        VmcsGuestNW::RSP.read().unwrap()
    }

    /// Set guest stack pointer. (`RSP`)
    pub fn set_stack_pointer(&mut self, rsp: usize) {
        VmcsGuestNW::RSP.write(rsp).unwrap()
    }

    /// Advance guest `RIP` by `instr_len` bytes.
    pub fn advance_rip(&mut self, instr_len: u8) -> AxResult {
        Ok(VmcsGuestNW::RIP
            .write(VmcsGuestNW::RIP.read().map_err(as_axerr)? + instr_len as usize)
            .map_err(as_axerr)?)
    }

    /// Add a virtual interrupt or exception to the pending events list,
    /// and try to inject it before later VM entries.
    pub fn inject_event(&mut self, vector: u8, err_code: Option<u32>) {
        self.pending_events.push_back((vector, err_code));
    }

    /// If enable, a VM exit occurs at the beginning of any instruction if
    /// `RFLAGS.IF` = 1 and there are no other blocking of interrupts.
    /// (see SDM, Vol. 3C, Section 24.4.2)
    pub fn set_interrupt_window(&mut self, enable: bool) -> AxResult {
        let mut ctrl = VmcsControl32::PRIMARY_PROCBASED_EXEC_CONTROLS
            .read()
            .map_err(as_axerr)?;
        let bits = vmcs::controls::PrimaryControls::INTERRUPT_WINDOW_EXITING.bits();
        if enable {
            ctrl |= bits
        } else {
            ctrl &= !bits
        }
        VmcsControl32::PRIMARY_PROCBASED_EXEC_CONTROLS
            .write(ctrl)
            .map_err(as_axerr)?;
        Ok(())
    }

    /// Returns the mutable reference of [`ApicTimer`].
    pub fn apic_timer_mut(&mut self) -> &mut ApicTimer<H> {
        &mut self.apic_timer
    }
}

// Implementation of private methods
impl<H: AxvmHal> VmxVcpu<H> {
    fn setup_msr_bitmap(&mut self) -> AxResult {
        // Intercept IA32_APIC_BASE MSR accesses
        let msr = x86::msr::IA32_APIC_BASE;
        self.msr_bitmap.set_read_intercept(msr, true);
        self.msr_bitmap.set_write_intercept(msr, true);
        // Intercept all x2APIC MSR accesses
        for msr in 0x800..=0x83f {
            self.msr_bitmap.set_read_intercept(msr, true);
            self.msr_bitmap.set_write_intercept(msr, true);
        }
        Ok(())
    }

    fn setup_vmcs(&mut self, entry: GuestPhysAddr, ept_root: HostPhysAddr) -> AxResult {
        let paddr = self.vmcs.phys_addr().as_usize() as u64;
        unsafe {
            vmx::vmclear(paddr).map_err(as_axerr)?;
            vmx::vmptrld(paddr).map_err(as_axerr)?;
        }
        self.setup_vmcs_host()?;
        self.setup_vmcs_guest(entry)?;
        self.setup_vmcs_control(ept_root)?;
        Ok(())
    }

    fn setup_vmcs_host(&mut self) -> AxResult {
        VmcsHost64::IA32_PAT
            .write(Msr::IA32_PAT.read())
            .map_err(as_axerr)?;
        VmcsHost64::IA32_EFER
            .write(Msr::IA32_EFER.read())
            .map_err(as_axerr)?;

        VmcsHostNW::CR0
            .write(Cr0::read_raw() as _)
            .map_err(as_axerr)?;
        VmcsHostNW::CR3
            .write(Cr3::read_raw().0.start_address().as_u64() as _)
            .map_err(as_axerr)?;
        VmcsHostNW::CR4
            .write(Cr4::read_raw() as _)
            .map_err(as_axerr)?;

        VmcsHost16::ES_SELECTOR
            .write(x86::segmentation::es().bits())
            .map_err(as_axerr)?;
        VmcsHost16::CS_SELECTOR
            .write(x86::segmentation::cs().bits())
            .map_err(as_axerr)?;
        VmcsHost16::SS_SELECTOR
            .write(x86::segmentation::ss().bits())
            .map_err(as_axerr)?;
        VmcsHost16::DS_SELECTOR
            .write(x86::segmentation::ds().bits())
            .map_err(as_axerr)?;
        VmcsHost16::FS_SELECTOR
            .write(x86::segmentation::fs().bits())
            .map_err(as_axerr)?;
        VmcsHost16::GS_SELECTOR
            .write(x86::segmentation::gs().bits())
            .map_err(as_axerr)?;
        VmcsHostNW::FS_BASE
            .write(Msr::IA32_FS_BASE.read() as _)
            .map_err(as_axerr)?;
        VmcsHostNW::GS_BASE
            .write(Msr::IA32_GS_BASE.read() as _)
            .map_err(as_axerr)?;

        let tr = unsafe { x86::task::tr() };
        let mut gdtp = DescriptorTablePointer::<u64>::default();
        let mut idtp = DescriptorTablePointer::<u64>::default();
        unsafe {
            dtables::sgdt(&mut gdtp);
            dtables::sidt(&mut idtp);
        }
        VmcsHost16::TR_SELECTOR.write(tr.bits()).map_err(as_axerr)?;
        VmcsHostNW::TR_BASE
            .write(get_tr_base(tr, &gdtp) as _)
            .map_err(as_axerr)?;
        VmcsHostNW::GDTR_BASE
            .write(gdtp.base as _)
            .map_err(as_axerr)?;
        VmcsHostNW::IDTR_BASE
            .write(idtp.base as _)
            .map_err(as_axerr)?;
        VmcsHostNW::RIP
            .write(Self::vmx_exit as usize)
            .map_err(as_axerr)?;

        VmcsHostNW::IA32_SYSENTER_ESP.write(0).map_err(as_axerr)?;
        VmcsHostNW::IA32_SYSENTER_EIP.write(0).map_err(as_axerr)?;
        VmcsHost32::IA32_SYSENTER_CS.write(0).map_err(as_axerr)?;

        Ok(())
    }

    fn setup_vmcs_guest(&mut self, entry: GuestPhysAddr) -> AxResult {
        // enable protected mode and paging.
        let cr0_guest = Cr0Flags::EXTENSION_TYPE | Cr0Flags::NUMERIC_ERROR;
        let cr0_host_owned =
            Cr0Flags::NUMERIC_ERROR | Cr0Flags::NOT_WRITE_THROUGH | Cr0Flags::CACHE_DISABLE;
        let cr0_read_shadow = Cr0Flags::NUMERIC_ERROR;
        VmcsGuestNW::CR0
            .write(cr0_guest.bits() as _)
            .map_err(as_axerr)?;
        VmcsControlNW::CR0_GUEST_HOST_MASK
            .write(cr0_host_owned.bits() as _)
            .map_err(as_axerr)?;
        VmcsControlNW::CR0_READ_SHADOW
            .write(cr0_read_shadow.bits() as _)
            .map_err(as_axerr)?;

        // enable physical address extensions that required in IA-32e mode.
        let cr4_guest = Cr4Flags::VIRTUAL_MACHINE_EXTENSIONS;
        let cr4_host_owned = Cr4Flags::VIRTUAL_MACHINE_EXTENSIONS;
        let cr4_read_shadow = 0;
        VmcsGuestNW::CR4
            .write(cr4_guest.bits() as _)
            .map_err(as_axerr)?;
        VmcsControlNW::CR4_GUEST_HOST_MASK
            .write(cr4_host_owned.bits() as _)
            .map_err(as_axerr)?;
        VmcsControlNW::CR4_READ_SHADOW
            .write(cr4_read_shadow)
            .map_err(as_axerr)?;

        macro_rules! set_guest_segment {
            ($seg: ident, $access_rights: expr) => {{
                use VmcsGuest16::*;
                use VmcsGuest32::*;
                use VmcsGuestNW::*;
                concat_idents!($seg, _SELECTOR).write(0).map_err(as_axerr)?;
                concat_idents!($seg, _BASE).write(0).map_err(as_axerr)?;
                concat_idents!($seg, _LIMIT)
                    .write(0xffff)
                    .map_err(as_axerr)?;
                concat_idents!($seg, _ACCESS_RIGHTS)
                    .write($access_rights)
                    .map_err(as_axerr)?;
            }};
        }

        set_guest_segment!(ES, 0x93); // 16-bit, present, data, read/write, accessed
        set_guest_segment!(CS, 0x9b); // 16-bit, present, code, exec/read, accessed
        set_guest_segment!(SS, 0x93);
        set_guest_segment!(DS, 0x93);
        set_guest_segment!(FS, 0x93);
        set_guest_segment!(GS, 0x93);
        set_guest_segment!(TR, 0x8b); // present, system, 32-bit TSS busy
        set_guest_segment!(LDTR, 0x82); // present, system, LDT

        VmcsGuestNW::GDTR_BASE.write(0).map_err(as_axerr)?;
        VmcsGuest32::GDTR_LIMIT.write(0xffff).map_err(as_axerr)?;
        VmcsGuestNW::IDTR_BASE.write(0).map_err(as_axerr)?;
        VmcsGuest32::IDTR_LIMIT.write(0xffff).map_err(as_axerr)?;

        VmcsGuestNW::CR3.write(0).map_err(as_axerr)?;
        VmcsGuestNW::DR7.write(0x400).map_err(as_axerr)?;
        VmcsGuestNW::RSP.write(0).map_err(as_axerr)?;
        VmcsGuestNW::RIP.write(entry).map_err(as_axerr)?;
        VmcsGuestNW::RFLAGS.write(0x2).map_err(as_axerr)?;
        VmcsGuestNW::PENDING_DBG_EXCEPTIONS
            .write(0)
            .map_err(as_axerr)?;
        VmcsGuestNW::IA32_SYSENTER_ESP.write(0).map_err(as_axerr)?;
        VmcsGuestNW::IA32_SYSENTER_EIP.write(0).map_err(as_axerr)?;
        VmcsGuest32::IA32_SYSENTER_CS.write(0).map_err(as_axerr)?;

        VmcsGuest32::INTERRUPTIBILITY_STATE
            .write(0)
            .map_err(as_axerr)?;
        VmcsGuest32::ACTIVITY_STATE.write(0).map_err(as_axerr)?;
        VmcsGuest32::VMX_PREEMPTION_TIMER_VALUE
            .write(0)
            .map_err(as_axerr)?;

        VmcsGuest64::LINK_PTR.write(u64::MAX).map_err(as_axerr)?; // SDM Vol. 3C, Section 24.4.2
        VmcsGuest64::IA32_DEBUGCTL.write(0).map_err(as_axerr)?;
        VmcsGuest64::IA32_PAT
            .write(Msr::IA32_PAT.read())
            .map_err(as_axerr)?;
        VmcsGuest64::IA32_EFER.write(0).map_err(as_axerr)?;
        Ok(())
    }

    fn setup_vmcs_control(&mut self, ept_root: HostPhysAddr) -> AxResult {
        // Intercept NMI and external interrupts.
        use super::vmcs::controls::*;
        use PinbasedControls as PinCtrl;
        vmcs::set_control(
            VmcsControl32::PINBASED_EXEC_CONTROLS,
            Msr::IA32_VMX_TRUE_PINBASED_CTLS,
            Msr::IA32_VMX_PINBASED_CTLS.read() as u32,
            (PinCtrl::NMI_EXITING | PinCtrl::EXTERNAL_INTERRUPT_EXITING).bits(),
            0,
        )?;

        // Intercept all I/O instructions, use MSR bitmaps, activate secondary controls,
        // disable CR3 load/store interception.
        use PrimaryControls as CpuCtrl;
        vmcs::set_control(
            VmcsControl32::PRIMARY_PROCBASED_EXEC_CONTROLS,
            Msr::IA32_VMX_TRUE_PROCBASED_CTLS,
            Msr::IA32_VMX_PROCBASED_CTLS.read() as u32,
            (CpuCtrl::UNCOND_IO_EXITING | CpuCtrl::USE_MSR_BITMAPS | CpuCtrl::SECONDARY_CONTROLS)
                .bits(),
            (CpuCtrl::CR3_LOAD_EXITING | CpuCtrl::CR3_STORE_EXITING).bits(),
        )?;

        // Enable EPT, RDTSCP, INVPCID, and unrestricted guest.
        use SecondaryControls as CpuCtrl2;
        vmcs::set_control(
            VmcsControl32::SECONDARY_PROCBASED_EXEC_CONTROLS,
            Msr::IA32_VMX_PROCBASED_CTLS2,
            0,
            (CpuCtrl2::ENABLE_EPT
                | CpuCtrl2::ENABLE_RDTSCP
                | CpuCtrl2::ENABLE_INVPCID
                | CpuCtrl2::UNRESTRICTED_GUEST)
                .bits(),
            0,
        )?;

        // Switch to 64-bit host, acknowledge interrupt info, switch IA32_PAT/IA32_EFER on VM exit.
        use ExitControls as ExitCtrl;
        vmcs::set_control(
            VmcsControl32::VMEXIT_CONTROLS,
            Msr::IA32_VMX_TRUE_EXIT_CTLS,
            Msr::IA32_VMX_EXIT_CTLS.read() as u32,
            (ExitCtrl::HOST_ADDRESS_SPACE_SIZE
                | ExitCtrl::ACK_INTERRUPT_ON_EXIT
                | ExitCtrl::SAVE_IA32_PAT
                | ExitCtrl::LOAD_IA32_PAT
                | ExitCtrl::SAVE_IA32_EFER
                | ExitCtrl::LOAD_IA32_EFER)
                .bits(),
            0,
        )?;

        // Load guest IA32_PAT/IA32_EFER on VM entry.
        use EntryControls as EntryCtrl;
        vmcs::set_control(
            VmcsControl32::VMENTRY_CONTROLS,
            Msr::IA32_VMX_TRUE_ENTRY_CTLS,
            Msr::IA32_VMX_ENTRY_CTLS.read() as u32,
            (EntryCtrl::LOAD_IA32_PAT | EntryCtrl::LOAD_IA32_EFER).bits(),
            0,
        )?;

        vmcs::set_ept_pointer(ept_root)?;

        // No MSR switches if hypervisor doesn't use and there is only one vCPU.
        VmcsControl32::VMEXIT_MSR_STORE_COUNT
            .write(0)
            .map_err(as_axerr)?;
        VmcsControl32::VMEXIT_MSR_LOAD_COUNT
            .write(0)
            .map_err(as_axerr)?;
        VmcsControl32::VMENTRY_MSR_LOAD_COUNT
            .write(0)
            .map_err(as_axerr)?;

        // Pass-through exceptions, don't use I/O bitmap, set MSR bitmaps.
        VmcsControl32::EXCEPTION_BITMAP.write(0).map_err(as_axerr)?;
        VmcsControl64::IO_BITMAP_A_ADDR.write(0).map_err(as_axerr)?;
        VmcsControl64::IO_BITMAP_B_ADDR.write(0).map_err(as_axerr)?;
        VmcsControl64::MSR_BITMAPS_ADDR
            .write(self.msr_bitmap.phys_addr().as_usize() as _)
            .map_err(as_axerr)?;
        Ok(())
    }

    #[naked]
    unsafe extern "C" fn vmx_launch(&mut self) -> ! {
        asm!(
            "mov    [rdi + {host_stack_top}], rsp", // save current RSP to Vcpu::host_stack_top
            "mov    rsp, rdi",                      // set RSP to guest regs area
            restore_regs_from_stack!(),
            "vmlaunch",
            "jmp    {failed}",
            host_stack_top = const size_of::<GeneralRegisters>(),
            failed = sym Self::vmx_entry_failed,
            options(noreturn),
        )
    }

    #[naked]
    unsafe extern "C" fn vmx_exit(&mut self) -> ! {
        asm!(
            save_regs_to_stack!(),
            "mov    r15, rsp",                      // save temporary RSP to r15
            "mov    rdi, rsp",                      // set the first arg to &Vcpu
            "mov    rsp, [rsp + {host_stack_top}]", // set RSP to Vcpu::host_stack_top
            "call   {vmexit_handler}",              // call vmexit_handler
            "mov    rsp, r15",                      // load temporary RSP from r15
            restore_regs_from_stack!(),
            "vmresume",
            "jmp    {failed}",
            host_stack_top = const size_of::<GeneralRegisters>(),
            vmexit_handler = sym Self::vmexit_handler,
            failed = sym Self::vmx_entry_failed,
            options(noreturn),
        );
    }

    fn vmx_entry_failed() -> ! {
        panic!("{}", vmcs::instruction_error().as_str())
    }

    /// Whether the guest interrupts are blocked. (SDM Vol. 3C, Section 24.4.2, Table 24-3)
    fn allow_interrupt(&self) -> bool {
        let rflags = VmcsGuestNW::RFLAGS.read().unwrap();
        let block_state = VmcsGuest32::INTERRUPTIBILITY_STATE.read().unwrap();
        rflags as u64 & x86_64::registers::rflags::RFlags::INTERRUPT_FLAG.bits() != 0
            && block_state == 0
    }

    /// Try to inject a pending event before next VM entry.
    fn check_pending_events(&mut self) -> AxResult {
        if let Some(event) = self.pending_events.front() {
            if event.0 < 32 || self.allow_interrupt() {
                // if it's an exception, or an interrupt that is not blocked, inject it directly.
                vmcs::inject_event(event.0, event.1)?;
                self.pending_events.pop_front();
            } else {
                // interrupts are blocked, enable interrupt-window exiting.
                self.set_interrupt_window(true)?;
            }
        }
        Ok(())
    }

    fn vmexit_handler(&mut self) {
        H::vmexit_handler(self);
        // Check if there is an APIC timer interrupt
        if self.apic_timer.check_interrupt() {
            self.inject_event(self.apic_timer.vector(), None);
        }
        self.check_pending_events().unwrap();
    }
}

impl<H: AxvmHal> Drop for VmxVcpu<H> {
    fn drop(&mut self) {
        unsafe { vmx::vmclear(self.vmcs.phys_addr().as_usize() as u64).unwrap() };
        info!("[AxVM] dropped VmxVcpu(vmcs: {:#x})", self.vmcs.phys_addr());
    }
}

fn get_tr_base(tr: SegmentSelector, gdt: &DescriptorTablePointer<u64>) -> u64 {
    let index = tr.index() as usize;
    let table_len = (gdt.limit as usize + 1) / core::mem::size_of::<u64>();
    let table = unsafe { core::slice::from_raw_parts(gdt.base, table_len) };
    let entry = table[index];
    if entry & (1 << 47) != 0 {
        // present
        let base_low = entry.get_bits(16..40) | entry.get_bits(56..64) << 24;
        let base_high = table[index + 1] & 0xffff_ffff;
        base_low | base_high << 32
    } else {
        // no present
        0
    }
}

impl<H: AxvmHal> Debug for VmxVcpu<H> {
    fn fmt(&self, f: &mut Formatter) -> Result {
        (|| -> AxResult<Result> {
            Ok(f.debug_struct("VmxVcpu")
                .field("guest_regs", &self.guest_regs)
                .field("rip", &VmcsGuestNW::RIP.read().map_err(as_axerr)?)
                .field("rsp", &VmcsGuestNW::RSP.read().map_err(as_axerr)?)
                .field("rflags", &VmcsGuestNW::RFLAGS.read().map_err(as_axerr)?)
                .field("cr0", &VmcsGuestNW::CR0.read().map_err(as_axerr)?)
                .field("cr3", &VmcsGuestNW::CR3.read().map_err(as_axerr)?)
                .field("cr4", &VmcsGuestNW::CR4.read().map_err(as_axerr)?)
                .field("cs", &VmcsGuest16::CS_SELECTOR.read().map_err(as_axerr)?)
                .field("fs_base", &VmcsGuestNW::FS_BASE.read().map_err(as_axerr)?)
                .field("gs_base", &VmcsGuestNW::GS_BASE.read().map_err(as_axerr)?)
                .field("tss", &VmcsGuest16::TR_SELECTOR.read().map_err(as_axerr)?)
                .finish())
        })()
        .unwrap()
    }
}
